# WebSocket Chat with Ollama LLM

A Proof of Concept (POC) demonstrating real-time chat with an AI using WebSockets, FastAPI, and Ollama.

> **ğŸ“– New to this project? Start with [SETUP.md](SETUP.md) for detailed setup instructions for any device!**

## Features

- ğŸš€ **Real-time Communication**: WebSocket-based chat for instant messaging
- ğŸ¤– **AI Integration**: Powered by Ollama with multiple LLM models
- ğŸ”„ **Model Selection**: Switch between different AI models on-the-fly
- ğŸ³ **Dockerized**: Complete containerized setup with Docker Compose
- ğŸ¨ **Modern UI**: Clean and responsive chat interface
- âš¡ **FastAPI Backend**: High-performance Python web framework

## Project Structure

```
websockets/
â”œâ”€â”€ app.py                 # FastAPI application with WebSocket endpoint
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ Dockerfile            # Container for FastAPI app
â”œâ”€â”€ docker-compose.yml    # Orchestrates Ollama and FastAPI services
â”œâ”€â”€ .env                  # Environment configuration (create from .env.example)
â”œâ”€â”€ .env.example          # Example environment configuration
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ index.html       # Chat interface
â”‚   â”œâ”€â”€ style.css        # Styling
â”‚   â””â”€â”€ script.js        # WebSocket client logic
â””â”€â”€ README.md
```

## Prerequisites

- Docker and Docker Compose installed on your system
- At least 4GB of free RAM (for Ollama and Llama2 model)
- Internet connection for initial setup

## Quick Start

### 1. Clone or navigate to the project directory

```bash
cd /Users/shivam/Desktop/workspace/poc/websockets
```

### 2. Configure environment variables

Copy the example environment file:

```bash
cp .env.example .env
```

Edit `.env` to customize your configuration:

```bash
# Application Configuration
FASTAPI_HOST=0.0.0.0
FASTAPI_PORT=8000

# Ollama Configuration
OLLAMA_HOST=http://ollama:11434
OLLAMA_MODEL=llama2              # Change to mistral, codellama, etc.
OLLAMA_TIMEOUT=120

# Docker Ports
FASTAPI_EXTERNAL_PORT=8000
OLLAMA_EXTERNAL_PORT=11434
```

### 3. Start the services

```bash
docker-compose up --build
```

This will:
- Pull the Ollama Docker image
- Download the model specified in `.env` (default: Llama2, ~4GB)
- Build the FastAPI application
- Start both services

**Note**: First startup will take several minutes to download the model.

### 4. Access the chat interface

Open your browser and go to:
```
http://localhost:8000
```

## Available AI Models

The application comes pre-configured with multiple AI models:

| Model | Size | Best For |
|-------|------|----------|
| **Gemma 2B** (default) | 1.7 GB | General conversations, good balance |
| **Phi-3 Mini** | 2.3 GB | Reasoning, technical questions |
| **Llama 3.2 1B** | 1.3 GB | Fast responses, lightweight |
| **Qwen 2.5 1.5B** | 934 MB | Multilingual support |

### Switching Models

1. Use the dropdown in the top-right corner of the chat interface
2. Select your desired model
3. Wait for it to load (first time only, ~30s-2min depending on model)
4. Start chatting once you see "loaded successfully"

### Pre-loading All Models (Optional)

To download all models at once:

```bash
./pull-all-models.sh
```

This downloads all 4 models (~6-8 GB total). Models are cached and load instantly after first download.

## Architecture

### System Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           User's Browser                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    Chat UI (http://localhost:8081)                â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚   Header    â”‚  â”‚    Model     â”‚  â”‚  Connection Status     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚   & Title   â”‚  â”‚   Selector   â”‚  â”‚      Indicator         â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚                                                             â”‚  â”‚  â”‚
â”‚  â”‚  â”‚              Chat Messages Area                            â”‚  â”‚  â”‚
â”‚  â”‚  â”‚   [User Message]                                           â”‚  â”‚  â”‚
â”‚  â”‚  â”‚              [AI Response]                                 â”‚  â”‚  â”‚
â”‚  â”‚  â”‚   [User Message]                                           â”‚  â”‚  â”‚
â”‚  â”‚  â”‚              [AI Response]                                 â”‚  â”‚  â”‚
â”‚  â”‚  â”‚                                                             â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚  â”‚
â”‚  â”‚  â”‚  Message Input Field         â”‚  â”‚ Send Button  â”‚            â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                  â–²                                       â”‚
â”‚                                  â”‚ HTML/CSS/JS                           â”‚
â”‚                                  â”‚ (static/)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                        WebSocket Connection (WSS/WS)
                                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Docker Container: fastapi_websocket                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                   FastAPI Application (app.py)                    â”‚  â”‚
â”‚  â”‚                                                                   â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚   WebSocket     â”‚  â”‚   REST API       â”‚  â”‚   Static File  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚   Endpoint      â”‚  â”‚   Endpoints      â”‚  â”‚   Serving      â”‚  â”‚  â”‚
â”‚  â”‚  â”‚   /ws           â”‚  â”‚   /health        â”‚  â”‚   /            â”‚  â”‚  â”‚
â”‚  â”‚  â”‚                 â”‚  â”‚   /api/models    â”‚  â”‚                â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Accept conn  â”‚  â”‚   /api/models/   â”‚  â”‚                â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Send/Receive â”‚  â”‚     load         â”‚  â”‚                â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Manage state â”‚  â”‚                  â”‚  â”‚                â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                                                                   â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚            Connection Manager                             â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Track active WebSocket connections                     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Broadcast messages                                     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Handle disconnections                                  â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                              â”‚                                    â”‚  â”‚
â”‚  â”‚                              â”‚ HTTP POST                          â”‚  â”‚
â”‚  â”‚                              â–¼                                    â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚          Ollama Query Handler                             â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Format prompts with system context                     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Configure parameters (temp, top_p, top_k)              â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Send to Ollama API                                     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Process responses                                      â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                    Port: 8081 (external) â†’ 8000 (internal)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                            HTTP REST API
                    (http://ollama:11434/api/generate)
                                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Docker Container: ollama                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                      Ollama Service                               â”‚  â”‚
â”‚  â”‚                                                                   â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚                   REST API Server                           â”‚ â”‚  â”‚
â”‚  â”‚  â”‚   â€¢ /api/generate  - Generate text from prompt              â”‚ â”‚  â”‚
â”‚  â”‚  â”‚   â€¢ /api/pull      - Download models                        â”‚ â”‚  â”‚
â”‚  â”‚  â”‚   â€¢ /api/list      - List available models                  â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                              â”‚                                    â”‚  â”‚
â”‚  â”‚                              â–¼                                    â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚                   Model Manager                             â”‚ â”‚  â”‚
â”‚  â”‚  â”‚   â€¢ Load models into memory                                 â”‚ â”‚  â”‚
â”‚  â”‚  â”‚   â€¢ Manage model lifecycle                                  â”‚ â”‚  â”‚
â”‚  â”‚  â”‚   â€¢ Handle concurrent requests                              â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                              â”‚                                    â”‚  â”‚
â”‚  â”‚                              â–¼                                    â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚               AI Models (LLM Inference)                     â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ Gemma 2B â”‚  â”‚  Phi-3   â”‚  â”‚ Llama    â”‚  â”‚  Qwen    â”‚   â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ 1.7 GB   â”‚  â”‚  2.3 GB  â”‚  â”‚ 3.2 1B   â”‚  â”‚ 2.5 1.5B â”‚   â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  â”‚          â”‚  â”‚          â”‚  â”‚  1.3 GB  â”‚  â”‚  934 MB  â”‚   â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                                                                   â”‚  â”‚
â”‚  â”‚  Memory Allocation: 8GB limit, 6GB reserved                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         Port: 11434                                      â”‚
â”‚                Volume: ollama_data (persistent model storage)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Docker Network: websockets_default                    â”‚
â”‚             (Internal communication between containers)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Breakdown

**1. Frontend (User Interface)**
- HTML/CSS/JavaScript single-page application
- WebSocket client for real-time communication
- Model selection dropdown
- Message display with typing indicators
- Connection status monitoring

**2. FastAPI Backend**
- WebSocket endpoint (`/ws`) for real-time chat
- REST API for model management
- Connection manager for multiple clients
- Request/response formatting
- Error handling and validation

**3. Ollama Engine**
- LLM model hosting and inference
- REST API for text generation
- Model management (pull, list, run)
- Memory-efficient model loading
- Concurrent request handling

### Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Browser â”‚â”€â”€â”€â”€ WebSocket â”€â”€â”€â”€â–¶â”‚  FastAPI â”‚â”€â”€â”€â”€ HTTP POST â”€â”€â”€â”€â–¶â”‚ Ollama  â”‚
â”‚         â”‚                      â”‚          â”‚                    â”‚         â”‚
â”‚  User   â”‚â—€â”€â”€â”€ WebSocket â”€â”€â”€â”€â”€â”‚  Server  â”‚â—€â”€â”€â”€ Response â”€â”€â”€â”€â”€â”€â”‚  LLM    â”‚
â”‚   UI    â”‚     (real-time)     â”‚          â”‚     (JSON)         â”‚ Engine  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                                â”‚                               â”‚
     â”‚                                â”‚                               â”‚
  Static                         app.py                          Models
  Files                         Python                          (AI)
```

### Message Flow Sequence

1. **User Input** â†’ User types message and clicks Send
2. **WebSocket Send** â†’ Message sent to FastAPI via WebSocket
3. **Prompt Formatting** â†’ FastAPI formats prompt with system context
4. **HTTP Request** â†’ FastAPI sends POST to Ollama API
5. **Model Inference** â†’ Ollama processes prompt with selected LLM
6. **Response Generation** â†’ AI generates response text
7. **JSON Response** â†’ Ollama returns JSON to FastAPI
8. **WebSocket Send** â†’ FastAPI forwards response via WebSocket
9. **UI Update** â†’ Browser displays AI response in chat

### Technology Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Frontend** | HTML5, CSS3, Vanilla JS | User interface |
| **Communication** | WebSockets | Real-time bidirectional messaging |
| **Backend Framework** | FastAPI (Python) | API server & WebSocket handler |
| **AI Engine** | Ollama | LLM hosting and inference |
| **Models** | Gemma, Phi-3, Llama, Qwen | Language models |
| **Containerization** | Docker + Docker Compose | Deployment & orchestration |
| **Networking** | Docker Network | Container communication |
| **Storage** | Docker Volumes | Persistent model storage |

## Configuration

### Environment Variables

All configuration is done through the `.env` file:

| Variable | Description | Default |
|----------|-------------|---------|
| `FASTAPI_HOST` | FastAPI server host | `0.0.0.0` |
| `FASTAPI_PORT` | FastAPI server port (internal) | `8000` |
| `FASTAPI_EXTERNAL_PORT` | FastAPI external port | `8000` |
| `OLLAMA_HOST` | Ollama API URL | `http://ollama:11434` |
| `OLLAMA_MODEL` | LLM model to use | `llama2` |
| `OLLAMA_TIMEOUT` | Request timeout in seconds | `120` |
| `OLLAMA_EXTERNAL_PORT` | Ollama external port | `11434` |

### Change the AI Model

Simply update the `OLLAMA_MODEL` variable in your `.env` file:

```bash
OLLAMA_MODEL=mistral
```

Then restart the services:

```bash
docker-compose down
docker-compose up --build
```

Available models: `llama2`, `mistral`, `codellama`, `phi`, etc.
See [Ollama library](https://ollama.ai/library) for more models.

### Adjust Ports

Update port numbers in your `.env` file:

```bash
FASTAPI_EXTERNAL_PORT=3000  # Access app on port 3000
OLLAMA_EXTERNAL_PORT=11435  # Ollama on port 11435
```

## API Endpoints

- `GET /` - Serves the chat interface
- `WebSocket /ws` - WebSocket endpoint for real-time chat
- `GET /health` - Health check endpoint
- `GET /api/models` - List available AI models
- `POST /api/models/load` - Load/switch to a different model

## Troubleshooting

### Ollama not responding

```bash
# Check if Ollama is running
docker ps

# View Ollama logs
docker logs ollama

# Restart services
docker-compose restart
```

### Model not downloaded

```bash
# Manually pull the model
docker exec -it ollama ollama pull llama2
```

### Port already in use

```bash
# Check what's using port 8000
lsof -i :8000

# Or change the port in docker-compose.yml
```

## Development

### Run locally without Docker

1. Install Ollama locally: https://ollama.ai/download

2. Create a `.env` file with local configuration:
```bash
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama2
OLLAMA_TIMEOUT=120
FASTAPI_HOST=0.0.0.0
FASTAPI_PORT=8000
```

3. Pull the model:
```bash
ollama pull llama2
```

4. Install Python dependencies:
```bash
pip install -r requirements.txt
```

5. Run the FastAPI server:
```bash
uvicorn app:app --reload
```

6. Access at `http://localhost:8000`

## Technologies Used

- **FastAPI**: Modern Python web framework
- **WebSockets**: Real-time bidirectional communication
- **Ollama**: Local LLM runtime
- **Llama2**: Meta's open-source language model
- **Docker**: Containerization
- **Vanilla JavaScript**: No frontend frameworks needed

## License

This is a POC/educational project. Feel free to use and modify as needed.

## Next Steps

Potential enhancements:
- Add conversation history
- Support multiple concurrent users
- Implement streaming responses
- Add user authentication
- Store chat history in database
- Add model selection in UI
- Implement rate limiting
- Add message formatting (markdown support)

## Resources

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Ollama Documentation](https://ollama.ai/docs)
- [WebSocket Protocol](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API)
